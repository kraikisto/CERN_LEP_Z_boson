{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f0be0e-a9ce-4561-8986-77f33d9db47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tracemalloc\n",
    "\n",
    "\n",
    "dataset91 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short91_f1_000.csv.gz',index_col=False)\n",
    "dataset92 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short92_e2_000.csv.gz',index_col=False)\n",
    "dataset93 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short93_d2_000.csv.gz',index_col=False)\n",
    "dataset94 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short94_c2_000.csv.gz',index_col=False)\n",
    "dataset = pd.concat([dataset91, dataset92, dataset93, dataset94])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da6b515-1cc1-4b4a-ba58-356c51cc82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_program(dataset):\n",
    "    #section 1: \n",
    "    lum1 = 711.1 #luminosity for this section\n",
    "    condition1 = np.abs(dataset.ECM-88.5) < 0.1 #ECM for this section is near 88.5\n",
    "    dataset.loc[condition1, \"LUM\"] = lum1 #set the luminosity under row \"LUM\"\n",
    "\n",
    "\n",
    "    #section 2: \n",
    "    lum2 = 632.7\n",
    "    condition2 = np.abs(dataset.ECM-89.5) < 0.1\n",
    "    dataset.loc[condition2, \"LUM\"] = lum2\n",
    "\n",
    "\n",
    "    #section 3: \n",
    "    lum3 = 622.6\n",
    "    condition3 = np.abs(dataset.ECM-90.2) < 0.1\n",
    "    dataset.loc[condition3 , \"LUM\"] = lum3\n",
    "\n",
    "\n",
    "    #section 4: \n",
    "    lum4 = 2482.5\n",
    "    condition4 = np.abs(dataset.ECM-91.2) < 0.1\n",
    "    dataset.loc[condition4, \"LUM\"] = lum4\n",
    "\n",
    "\n",
    "    #section 5: \n",
    "    lum5 = 666.1\n",
    "    condition5 = np.abs(dataset.ECM-92) < 0.1\n",
    "    dataset.loc[condition5, \"LUM\"] = lum5\n",
    "\n",
    "\n",
    "    #section 6: \n",
    "    lum6 = 634.6\n",
    "    condition6 = np.abs(dataset.ECM-93) < 0.1\n",
    "    dataset.loc[condition6, \"LUM\"] = lum6\n",
    "\n",
    "\n",
    "    #section 7: \n",
    "    lum7 = 681.2\n",
    "    condition7 = np.abs(dataset.ECM-93.7) < 0.1\n",
    "    dataset.loc[condition7, \"LUM\"] = lum7\n",
    "    \n",
    "    \n",
    "    \n",
    "def second_program(dataset):\n",
    "    dataset_sorted = dataset.sort_values(by=[\"ECM\"])\n",
    "    dataset_sorted = dataset_sorted.reset_index(drop=True)\n",
    "    unique_values = dataset_sorted.ECM.unique() #simulate having to print\n",
    "\n",
    "    #section 1: \n",
    "    index1 = dataset_sorted[dataset_sorted.ECM == 89.506].index[0]\n",
    "    lum1 = 711.1 #luminosity for this section\n",
    "    dataset_sorted.loc[:index1, \"LUM\"] = lum1 #set the luminosity under row \"LUM\"\n",
    "\n",
    "\n",
    "    #section 2: \n",
    "    lum2 = 632.7\n",
    "    index2 = dataset_sorted[dataset_sorted.ECM == 90.256].index[0]\n",
    "    dataset_sorted.loc[index1:index2, \"LUM\"] = lum2\n",
    "\n",
    "\n",
    "    #section 3: \n",
    "    lum3 = 622.6\n",
    "    index3 = dataset_sorted[dataset_sorted.ECM == 91.25].index[0]\n",
    "    dataset_sorted.loc[index2:index3, \"LUM\"] = lum3\n",
    "\n",
    "\n",
    "    #section 4: \n",
    "    lum4 = 2482.5\n",
    "    index4 = dataset_sorted[dataset_sorted.ECM == 92.004].index[0]\n",
    "    dataset_sorted.loc[index3:index4, \"LUM\"] = lum4\n",
    "\n",
    "\n",
    "    #section 5: \n",
    "    lum5 = 666.1\n",
    "    index5 = dataset_sorted[dataset_sorted.ECM == 93.015].index[0]\n",
    "    dataset_sorted.loc[index4:index5, \"LUM\"] = lum5\n",
    "\n",
    "\n",
    "    #section 6: \n",
    "    lum6 = 634.6\n",
    "    index6 = dataset_sorted[dataset_sorted.ECM == 93.765].index[0]\n",
    "    dataset_sorted.loc[index5:index6, \"LUM\"] = lum6\n",
    "\n",
    "\n",
    "    #section 7: \n",
    "    lum7 = 681.2\n",
    "    dataset_sorted.loc[index6:, \"LUM\"] = lum7\n",
    "    return dataset_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6d57d4-66ef-42be-8e7b-08134ccb6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018329620361328125\n",
      "0.016561031341552734\n",
      "0.012064695358276367\n",
      "0.026291608810424805\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "#measure first program in small dataset\n",
    "start_time = time.time()\n",
    "first_program(dataset91)\n",
    "end_time = time.time()\n",
    "result1 = end_time-start_time\n",
    "\n",
    "#measure second program in small dataset\n",
    "start_time = time.time()\n",
    "second_program(dataset91)\n",
    "end_time = time.time()\n",
    "result2 = end_time-start_time\n",
    "\n",
    "#measure first program in bigger dataset\n",
    "start_time = time.time()\n",
    "first_program(dataset)\n",
    "end_time = time.time()\n",
    "result3 = end_time-start_time\n",
    "\n",
    "#measure second program in bigger dataset\n",
    "start_time = time.time()\n",
    "second_program(dataset)\n",
    "end_time = time.time()\n",
    "result4 = end_time-start_time\n",
    "\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a6a60-02cd-4444-8734-d075f2938bbd",
   "metadata": {},
   "source": [
    "This isn't the most efficient method of testing the speed of an algorithm, since it just takes the real times and subtracts them. You should run it few times, but you will get a basic idea of how the programs compare to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a69d17-7d35-44d4-a740-7e1858462063",
   "metadata": {},
   "source": [
    "So for the smaller dataset the first one is often slower, but for the larger its always faster, why is this? Well lets look at the programs starting with the lines: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5274c2-b0fb-4230-a152-aa17657c8eee",
   "metadata": {},
   "source": [
    "`condition1 = np.abs(dataset.ECM-88.5) < 0.1` program one\n",
    "\n",
    "`index1 = dataset[dataset.ECM == 89.478].index[0]` program two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3879fa7-9e6c-47e1-88b3-66e25a223ac1",
   "metadata": {},
   "source": [
    "Both are actually quite similar, running through the entire data checking the codition for each value. Let's say we have $N$ number of datapoints in our data. Then going through would take $aN$ time where $a$ is the amount of time it takes to go through one and is a constant. We usually denote this as $O(N)$ time complexity.  \n",
    "\n",
    "For the next lines: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a772a-8f1e-4aa2-bfff-729ea3fdc86e",
   "metadata": {},
   "source": [
    "`dataset.loc[condition1, \"LUM\"] = 711.1` program one\n",
    "\n",
    "`dataset.loc[:index1, \"LUM\"] = 711.1` program two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093b805-2368-4603-8727-a809e5416abd",
   "metadata": {},
   "source": [
    "Here program one is actually worse. It goes through the entire data checking the whether the condition is True or False just like the last lines we looked at. This is slower than actually giving the indices that need to be edited. Since program 2 goes through all datapoints once, instead of 7 times its 7 times faster. Both programs are still $O(N)$ complexity since both runtimes grow linearly in terms of $N$, one is just always 7 times slower. \n",
    "\n",
    "But now the reason why program 2 is worse on a large dataset: It has to sort the data first. There are a bunch of different sorting algorithms, but this uses quicksort as a default which is $O(N log N)$. This is larger than any of the other parts of the program so when the amount of data grows this will grow faster than all the other parts explaining why for a larger dataset the second program is always slower. \n",
    "\n",
    "The last 2 parts which are the resetting of indices and calculation of unique values are both $O(N)$ for the same reasons we covered before. Now the complexity of the entire program is just the largest term which is $O(N log N)$.\n",
    "\n",
    "Now what about the memory usage? The tracemalloc library can be used to test the memory usage. Now when testing it is important to remember not to have anything already saved before for accurate measurements. The safest way to do that is to restart your kernel and run the first 2 cells to have everything you need imported and saved. After that you can run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1bc706-cc58-4f1a-912b-7a8d0337c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10153, 573705)\n",
      "(155310, 16483973)\n"
     ]
    }
   ],
   "source": [
    "tracemalloc.start()\n",
    "first_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "\n",
    "tracemalloc.start()\n",
    "second_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3238a16-0348-4b1c-b315-701465d7c38c",
   "metadata": {},
   "source": [
    "The first value given is the current memory usage of the program and the second the peak memory usage. The peak is what matters here since that is what can stop your program from running. Now clearly the first program is the better one, but why? If we look at what variables we actually save, its quite obvious. In the beginning we save the entire dataset, sorted to a different order and then save the entire dataset again just with different indices. An easy way to make sure which part of the code is using too much memory is to compare just that part to the entire program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99854a80-b382-4d2b-9f11-0f826bba4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303154, 16630625)\n",
      "(4457122, 16482811)\n"
     ]
    }
   ],
   "source": [
    "tracemalloc.start()\n",
    "second_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "\n",
    "tracemalloc.start()\n",
    "dataset_sorted = dataset.sort_values([\"ECM\"])\n",
    "dataset_sorted = dataset_sorted.reset_index(drop=True)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef2a03-429c-4329-a868-706363f9ad58",
   "metadata": {},
   "source": [
    "The peak is almost the same and current usage is ten times higher. This is clearly the main problem, but how do we fix it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be32abf-3d38-4f60-8927-3e71e80aaf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266297, 15801264)\n",
      "(4647288, 12208766)\n"
     ]
    }
   ],
   "source": [
    "def second_program_improved(dataset):\n",
    "    dataset.sort_values(by=[\"ECM\"], inplace = True)\n",
    "    dataset.reset_index(drop=True, inplace = True)\n",
    "    unique_values = dataset.ECM.unique() #simulate having to print\n",
    "\n",
    "    #section 1:\n",
    "    lum1 = 711.1 #luminosity for this section\n",
    "    index1 = dataset[dataset.ECM == 89.506].index[0]\n",
    "    dataset.loc[:index1, \"LUM\"] = lum1 #set the luminosity under row \"LUM\"\n",
    "\n",
    "\n",
    "    #section 2: \n",
    "    lum2 = 632.7\n",
    "    index2 = dataset[dataset.ECM == 90.256].index[0]\n",
    "    dataset.loc[index1:index2, \"LUM\"] = lum2\n",
    "\n",
    "\n",
    "    #section 3: \n",
    "    lum3 = 622.6\n",
    "    index3 = dataset[dataset.ECM == 91.25].index[0]\n",
    "    dataset.loc[index2:index3, \"LUM\"] = lum3\n",
    "\n",
    "\n",
    "    #section 4: \n",
    "    lum4 = 2482.5\n",
    "    index4 = dataset[dataset.ECM == 92.004].index[0]\n",
    "    dataset.loc[index3:index4, \"LUM\"] = lum4\n",
    "\n",
    "\n",
    "    #section 5: \n",
    "    lum5 = 666.1\n",
    "    index5 = dataset[dataset.ECM == 93.015].index[0]\n",
    "    dataset.loc[index4:index5, \"LUM\"] = lum5\n",
    "\n",
    "\n",
    "    #section 6: \n",
    "    lum6 = 634.6\n",
    "    index6 = dataset[dataset.ECM == 93.765].index[0]\n",
    "    dataset.loc[index5:index6, \"LUM\"] = lum6\n",
    "\n",
    "\n",
    "    #section 7: \n",
    "    lum7 = 681.2\n",
    "    dataset.loc[index6:, \"LUM\"] = lum7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tracemalloc.start()\n",
    "second_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "\n",
    "tracemalloc.start()\n",
    "second_program_improved(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8d8ce-c428-49b9-ae50-827ff5174278",
   "metadata": {},
   "source": [
    "It's important to remember that now this function will change the dataset that we give it and sometimes this is not adviced. However, generally when dealing with large sets of data, it's recommended to edit the existing dataset instead of creating a copy to edit, unless you know that you will need the original again. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
