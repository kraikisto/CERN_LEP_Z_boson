{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f10e2f-b0c6-4bd6-a564-e5b66c694dc1",
   "metadata": {},
   "source": [
    "Welcome to an exercise where we will trying to determine the mass of the Z boson using data from the Large Electron-Positron collider (LEP). In fact, the measurements of the Z and the W bosons' masses are the most important research done at LEP. The main problem with this the Z boson is the actual detection. Since Z bosons lifetime is very short it will decay into other particles, before we can detect it. One of the possible products of the decay can be 2 muons, which we can detect and will focus on. Here we will only use data of these double muons that come from Z particles. From the muons we can work backwards and calculate the mass of the particle that they decayed from. The mass values will vary quite a bit, but we have some methods that we can use to take out bad datapoints. \n",
    "\n",
    "Start with the necessary imports and reading the datasets of github. The datasets can be combined using the `concat()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d153510-cb21-47e7-8935-bc8c0c519172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import time\n",
    "\n",
    "dataset91 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short91_f1_000.csv.gz',index_col=False)\n",
    "dataset92 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short92_e2_000.csv.gz',index_col=False)\n",
    "dataset93 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short93_d2_000.csv.gz',index_col=False)\n",
    "dataset94 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short94_c2_000.csv.gz',index_col=False)\n",
    "dataset = pd.concat([dataset91, dataset92, dataset93, dataset94])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644594a-c8d8-4cf1-a0e8-a925bb9ea9ab",
   "metadata": {},
   "source": [
    "Z bosons decay is a relativistic process, so we can't solve its mass just by adding up the masses of the products, since some of the mass can turn into energy. We need to define a new definition of mass, which we can get from relativistic theory: Invariant mass. In particle physics it's commonly defined as \n",
    "\n",
    "$$\n",
    "M^2 c^2 = \\dfrac{E^2}{c^2} - ||p||^2\n",
    "$$\n",
    "\n",
    "\n",
    "Because both energy and momentum are conserved, the invariant mass is also conserved and therefore we can calculate the invariant mass of the Z boson by calculating the invariant mass of the products. To calculate the combined invariant masses we can use the formula:\n",
    "\n",
    "$$\n",
    "M = \\sqrt{(E_1 + E_2)^2 - c^2 (\\vec{p}_1 + \\vec{p}_2)^2 }\n",
    "$$\n",
    "\n",
    "To simplify the calculations we can use natural units where $c=1$. If you just do this the program will give you a warning about negative values under the square root. Due to measurement error some values will infact be negative. Since they are caused by measurement error they are all quite small and will just be removed for now. \n",
    "\n",
    "If you try to do that, you'll first notice an error caused by mistakes in the data. After some tests you'll notice that there are some values of PZ1 with the value `**********`. First we remove those from the dataset and then change the datatype of the values to float64 since the values were saved as strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda403a6-20dc-4f31-959d-63b58a23fd83",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1: Invariant mass\n",
    "\n",
    "Your first job is to add a new column to the dataset with the invariant mass values. The before mentioned problems in the data have been fixed already, but the negative values still need to be removed. Add a condition to remove them. Use the equation above to calculate the mass values. The values from the dataset can be accessed simply by writing `dataset.E1` or other column name. The built-in operations of pandas are generally recommended as they are optimised for speed and memory usage. If you want to learn more about this check out the extra material below:\n",
    "\n",
    "<details>\n",
    "<summary> Pandas performance comparison </summary>\n",
    "Is pandas really faster? Well the answer is most of the time. A bunch of the more advanced python librabries like numpy and pandas use vectorisation the speed up their functions. In vectorisation, instead of running through the values one by one, we perform the same operation on a bunch of values saved as a vector. This is possible because of modern CPUs single instruction, multiple data (SIMD) processing. Traditionally processors could only hold one value and thus had to process one value at a time. Modern processors can hold multiple values and therefore run the same operation on a vector of values if you program it to do so. Let's now demonstrate this is a simple example comparing a columns of random data:\n",
    "\n",
    "Let's use a simple example and compare the values from 2 columns in three different ways:\n",
    "\n",
    "`df[fd.A != df.B]`  pandas using vectorisation\n",
    "\n",
    "`df[[x != y for x, y in zip(df.A, df.B)]]` Using list comprehension, which is essentially a built-in optimised for-loop\n",
    "    \n",
    "A normal for-loop not optimised in any way\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/comparison_alt.png)\n",
    "\n",
    "We have plotted the measurements in logarithmic scale, but if you were to show them in linear scale, the lines would in fact be straight. A loop over a dataset does in fact always grow linearly in runtime and vectorisation doesn't change that, but it does significantly affect the slope of the graph. You can also notice that the basic for-loop is actually faster for small datasets, showing that you can't test functions just with small datasets and assume they behave the same way with bigger ones. Interestingly if we compare to another vectorised method from numpy we see that it is even faster. It seems that specifically `.values` is signifantly better than its pandas counterpart:\n",
    "\n",
    "`df[df.A.values != df.B.values]` \n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/pandasvnumpy.png)\n",
    "\n",
    "Here it's clear that the slope of the graph is the same and rather the initial runtime is changed. Vectorisation isn't always even better. One example of this is flattening lists, meaning turning a list with multiple dimensions into a 1-dimensional list. This is most common with turning a matrix into a list. Here we'll compare `stack`function from pandas to list comprehension and to `chain`from the `itertools` python module. \n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/list_flattening.png)\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "#this uses perfplot, a tool specifically made for performance testing\n",
    "import perfplot  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def normal_loop(df):\n",
    "    list1 = list(df.A)\n",
    "    list2 = list(df.B)\n",
    "    for i in range(0,len(list1)):\n",
    "        if list1[i] == list2[i]:\n",
    "            df = df.drop(index = i)\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "# Boolean indexing with float value comparison.\n",
    "perfplot.show(\n",
    "    setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=['A','B']),\n",
    "    kernels=[\n",
    "        lambda df: df[df.A != df.B],\n",
    "        lambda df: df[[x != y for x, y in zip(df.A, df.B)]],\n",
    "        lambda df: normal_loop(df),\n",
    "    ],\n",
    "    labels=['vectorised', 'list comp', \"normal loop\"],\n",
    "    n_range=[2**k for k in range(5, 22)],\n",
    "    xlabel='N', \n",
    "    logy=False, \n",
    "    logx=False\n",
    ")\n",
    "\n",
    "\n",
    "#pandas vs numpy\n",
    "perfplot.show(\n",
    "    setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=['A','B']),\n",
    "    kernels=[\n",
    "        lambda df: df[df.A != df.B],\n",
    "        lambda df: df[df.A.values != df.B.values],\n",
    "\n",
    "    ],\n",
    "    labels=['pandas', 'numpy'],\n",
    "    n_range=[2**k for k in range(0, 15)],\n",
    "    xlabel='N', \n",
    "    logy=False, \n",
    "    logx=False\n",
    ")\n",
    "\n",
    "\n",
    "# Nested list flattening.\n",
    "perfplot.show(\n",
    "    setup=lambda n: pd.concat([pd.Series([['a', 'b', 'c'], [1, 2], []])] * n, ignore_index=True),\n",
    "    kernels=[\n",
    "        lambda ser: pd.DataFrame(ser.tolist()).stack().reset_index(drop=True),\n",
    "        lambda ser: pd.Series(list(chain.from_iterable(ser.tolist()))),\n",
    "        lambda ser: pd.Series([y for x in ser for y in x]),\n",
    "    ],\n",
    "    labels=['pandas.stack', 'itertools.chain', 'nested list comp'],\n",
    "    n_range=[2**k for k in range(0, 15)],\n",
    "    xlabel='N',    \n",
    "    equality_check=None,\n",
    "    logy=False,\n",
    "    logx=False    \n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57896a-95bd-4e80-80e1-af75acaf91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.PZ1 != '**********'] #removes datapoints where PZ1 is **********\n",
    "dataset = dataset.astype({'PZ1': 'float64'}) #convert the strings to floats for the upcoming calculations\n",
    "\n",
    "#removing negative values:\n",
    "dataset = dataset[...] #similarly as in the first line, with a different inequality\n",
    "\n",
    "#calculating invariant mass:\n",
    "dataset[\"M\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04743aa2-2ca6-4111-a952-bbd07709a9f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise 2: Histogram\n",
    "\n",
    "Now let's plot the histogram of the invariant mass values. It can be done using `plt.his()`. The argumets for it are:\n",
    "\n",
    "- x=array of all datapoints\n",
    "- bins=number of bars in the histogram\n",
    "- range=limits for the values in the histogram, given as a tuple of the 2 limits\n",
    "\n",
    "You will know that you have done it correctly if you see a spike around the mass of Z boson. At the time the mass of the Z boson was measured at around 90 MeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c741892-bfc1-438b-8ba1-7d93c71d7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.hist(x=..., bins=..., range=(..., ...))\n",
    "\n",
    "plt.xlabel('Invariant mass [GeV/c²]', fontsize=15)\n",
    "plt.ylabel('Number of events', fontsize=15)\n",
    "plt.title('Invariant mass', fontsize=15) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6096b-5894-41e9-81c8-a1abebf732e3",
   "metadata": {},
   "source": [
    "Now that we have the histogram we can start analysing it. There is a peak (often called a resonance), which implies that we have been producing particles with that invariant mass. We can actually get even more from this histogram. This is done by plotting what is called a Breit-Wigner function on the histogram. Breit-Wigner distribution is commonly used to model unstable particles in high energy physics and is of the form:\n",
    "\n",
    "$$\n",
    "f(E) = \\dfrac{k}{(E^2 - M^2)^2 + M^2 \\Gamma^2} \n",
    "$$\n",
    "\n",
    "where $k = \\dfrac{2\\sqrt{2}M \\Gamma \\gamma}{\\pi\\sqrt{M^2 + \\gamma}}$ with $\\gamma = \\sqrt{M^2(M^2 + \\Gamma^2)}$ which are in natural units $\\hbar = c = 1$. $\\Gamma$ is called the decay width or full width at half maximum and describes the width of the peak. More specifically it is the distance between the points in the graph that are half of the maximum value. The derivation for this distribution is quite complex, but you can see a simplified version of it below: \n",
    "\n",
    "<details>\n",
    "<summary>Derivation of the Breit Wigner distributions</summary>\n",
    "    \n",
    "    \n",
    "### Non-relativistic Breit-Wigner distribution\n",
    "\n",
    "Let's start with the non-relativistic version of the Breit-Wigner distribution. We can derive it using basic quantum mechanics:\n",
    "\n",
    "To begin with, let's say the Z boson has a wave function $|\\psi(t) \\rangle$. Its evolution is given by the schrödinger equation:\n",
    "\n",
    "$$\n",
    "i \\hbar \\frac{d}{dt} |\\psi(t) \\rangle = H |\\psi(t) \\rangle\n",
    "$$\n",
    "\n",
    "Since the Hamiltonian $H$ isn't time dependent it has a solution: $|\\psi(t) \\rangle = e^{-i H t/ \\hbar} |\\psi(0) \\rangle$. Since the Hamiltionian gives us the energy of the system, let's look at that. We can assume that there are no outside forces that would affect the Z boson, therefore it keeps a constant energy $M_0$, until decaying. Therefore we get:\n",
    "\n",
    "$$\n",
    "|\\psi(t) \\rangle = e^{-i M_0 t/ \\hbar} e^{-t/2\\tau} |\\psi(0) \\rangle\n",
    "$$\n",
    "\n",
    "where $\\tau$ is mean lifetime for Z boson. Now we can convert this into the energy domain by doing a Fourier transformation:\n",
    "\n",
    "$$\n",
    "f(E) = \\int \\limits_0^\\infty dt \\psi(t) e^{iEt}\n",
    "$$\n",
    "\n",
    "This is just a simple integral, which gives us $f(E) = \\frac{i \\psi(0)}{(M_0−E) − \\frac{i}{2 \\tau}}$. Now we can get the probability distribution simply by calculating:\n",
    "\n",
    "$$\n",
    "p(E) = |f(E)|^2 = \\frac{|\\psi(0)|^2}{(M_0−E)^2 + \\frac{1}{4 \\tau^2}} \n",
    "$$\n",
    "\n",
    "    \n",
    "    \n",
    "### Relativistic Breit-Wigner distribution\n",
    "\n",
    "Since our particles are moving at relativistic speeds, we need to use the relativistic Breit-Wigner distribution. This one is a lot more difficult to derive as it comes from Quantum field theory. Since the proper derivation is quite complex, we'll look at the problem through Feynman diagrams and skip some details, like the Z boson having spin. Let's start with the basic feynman diagram for the scattering of the Z boson:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/1st_degree_FD.png)\n",
    "\n",
    "Here the internal propagator, the wavy line, is the propagator for the Z boson. The propagator for this has the value $\\frac{i}{E^2 - M^2}$. However, this trivial Feynman diagram isn't enough. The first correction we need to add is a single loop:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/2nd_degree_FD.png)\n",
    "\n",
    "This propagator has a value of $\\frac{i}{E^2 - M^2} i \\Sigma \\frac{i}{E^2 - M^2}$. Next the feynman diagrams with 2 loops which there are 2: \n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/3rd_degree_FD.png)\n",
    "\n",
    "These have a propagator: $\\frac{i}{E^2 - M^2} i \\Sigma \\frac{i}{E^2 - M^2} i \\Sigma \\frac{i}{E^2 - M^2}$.  At this point, you can see the pattern. We can continue this and when we take the sum of all the diagrams, its clearly just a geometric series, giving us the total propagator:\n",
    "\n",
    "$$\n",
    "\\frac{i}{E^2 - M^2 + \\Sigma}\n",
    "$$\n",
    "\n",
    "Using the optical theorem, which we will not cover here, we get a value for $\\Sigma = i M \\Gamma$ and get the propagator: \n",
    "\n",
    "$$\n",
    "\\Delta_{BW} = \\frac{i}{E^2 - M^2 + i M \\Gamma} = \\frac{M \\Gamma}{(E^2 - M^2)^2 + M^2 \\Gamma^2} + i\\frac{E^2 - M^2}{(E^2 - M^2)^2 + M^2 \\Gamma^2}\n",
    "$$\n",
    "\n",
    "This is the Breit-Wigner propagator. The probability distribution is just the propagator squared:\n",
    "\n",
    "$$\n",
    "|\\Delta_{BW}|^2 = (\\frac{M \\Gamma}{(E^2 - M^2)^2 + M^2 \\Gamma^2})^2 + (\\frac{E^2 - M^2}{(E^2 - M^2)^2 + M^2 \\Gamma^2})^2 = \\frac{(E^2 - M^2)^2 + M^2 \\Gamma^2}{((E^2 - M^2)^2 + M^2 \\Gamma^2)^2} = \\frac{1}{(E^2 - M^2)^2 + M^2 \\Gamma^2}\n",
    "$$\n",
    "\n",
    "Lastly, for this to be a probability distribution, we have to normalise it. This is done by adding a constant $k$ to the numerator:\n",
    "\n",
    "$$\n",
    "f(E) = \\frac{k}{(E^2 - M^2)^2 + M^2 \\Gamma^2}\n",
    "$$\n",
    "\n",
    "where the constant has to be $k = \\dfrac{2\\sqrt{2}M \\Gamma \\gamma}{\\pi\\sqrt{M^2 + \\gamma}}$ with $\\gamma = \\sqrt{M^2(M^2 + \\Gamma^2)}$ for the distribution to be normalised. \n",
    "\n",
    "\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403b094-62b7-4941-888a-117db96f07ff",
   "metadata": {},
   "source": [
    "Let's start with the simple parts of the fitting. First we limit ourselves to the area around where we know the particle is by setting limits to the mass values and remove the rest from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f0416-7c97-4bdc-b9d3-1154a03d7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit to where Z-boson should be:\n",
    "lowerlimit = 70\n",
    "upperlimit = 100\n",
    "bins = 50\n",
    "\n",
    "#Create a histogram only around the peak\n",
    "histogram = plt.hist(dataset.M, bins=bins, range=(lowerlimit,upperlimit), density = True) \n",
    "plt.close() #stop from showing the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f8bd7-c1b9-49fc-bc18-e54bfd296204",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise 3: Fitting\n",
    "\n",
    "In this exercise you will create the fit using `curve_fit()` from the `scipy.optimize` library which has the arguments:\n",
    "\n",
    "- f=the form of function that we try to fit (e.g exponential function, linear function,...)\n",
    "- xdata=x-coordinates of the points\n",
    "- ydata=y-coordinates of th points initial values \n",
    "- p0=initial guesses, which the program will start guessing from\n",
    "- sigma=magnitude of uncertainty, which is used to determine covariance, here it's $\\sqrt{(y)}$\n",
    "\n",
    "The values are mostly given to you, you will just have to add some of the initial guesses based on the previous histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfe310-845b-40cc-b878-b2b9ed50d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add initial guesses (3 are already given):\n",
    "p0 = [decay width, peak, 4, 2, 4]\n",
    "\n",
    "def breitwigner(E, gamma, M, a, b, A):\n",
    "    return a*E+b+A*( (2*np.sqrt(2)*M*gamma*np.sqrt(M**2*(M**2+gamma**2)))/(np.pi*np.sqrt(M**2+np.sqrt(M**2*(M**2+gamma**2)))) )/((E**2-M**2)**2+M**2*gamma**2)\n",
    "\n",
    "#Histogram as an object consists of 2 lists: List of the heights of the bars and the edges of the bins\n",
    "#Using normal list operations we can use the histogram to get x and y coordinates:\n",
    "y = histogram[0] #gives the height of each bar in a list\n",
    "x = 0.5*(histogram[1][0:-1] + histogram[1][1:]) #list of midpoints of a bins\n",
    "\n",
    "#We save both the best values and the covariance from the fit\n",
    "best, covariance = curve_fit(f=..., xdata= ..., ydata=..., p0 = ..., sigma= ...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011243bf-a54a-4f4a-9e93-4ab94993d354",
   "metadata": {},
   "source": [
    "Lastly we plot both this fitted function and the histogram into the same picture. We can also print the best values and their limits of error, which can be easily calculated from the covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd491f-4dc0-410e-8072-f010e3100760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covariance is is determined using the sigma given. The diagonal elements are the variances.\n",
    "error = np.sqrt(np.diag(covariance))\n",
    "    \n",
    "#Parameters and their errors from optimization:\n",
    "print(\"The values and the uncertainties from the optimization:\")\n",
    "print(\"\")\n",
    "#Here is also a useful way to print out strings with variables in them\n",
    "print(f\"The value of the decay width = {best[0]} +- {error[0]}\")\n",
    "print(f\"The value of the maximum of the distribution = {best[1]} +- {error[1]}\")\n",
    "print(f\"a = {best[2]} +- {error[2]}\")\n",
    "print(f\"b = {best[3]} +- {error[3]}\")\n",
    "print(f\"A = {best[4]} +- {error[4]}\")\n",
    "\n",
    "#plot both graphs:\n",
    "plt.hist(dataset.M, bins=bins, range=(lowerlimit,upperlimit), density = True)\n",
    "plt.plot(x, breitwigner(x, *best), 'r-', label=f'gamma = {best[0]}, M = {best[1]}')\n",
    "plt.xlabel('Invariant mass [GeV]')\n",
    "plt.ylabel('Number of events')\n",
    "plt.title('The Breit-Wigner fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b00d3-5bc6-4165-8993-67a1f6c8d8a6",
   "metadata": {},
   "source": [
    "Well what do we get out of this? We got a more accurate estimation for the invariant mass of the Z boson. We also now know the value of $\\Gamma$. It is also connected to mean lifetime of the particle which the resonance correlates to. The mean lifetime can be calculated from the formula:\n",
    "\n",
    "$$\n",
    "\\Gamma = \\dfrac{\\hbar}{\\tau}\n",
    "$$\n",
    "\n",
    "where $\\hbar$ is the reduced Planck constant and $\\tau$ the lifetime of the particle. Here the $\\Gamma$ is resonance width for the entire decay, not just for the decay to muons. Therefore, to get the lifetime of the Z boson, we would need to add up all of the decay widths.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e00a3e-0034-4483-9d06-13d224c10cd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our measurement for the invariant mass is still quite inaccurate. A large missing potential is a indicator that the datapoint may be inaccurate. Let's remove ones with a large missing potential and see if it makes the results more accurate:\n",
    "\n",
    "#### Exercise 4: Missing momentum\n",
    "\n",
    "Here you will have to remove large missing momentum from the dataset. You will have to experiment with the limit and see if you can improve the measurement of the Z boson mass. For this exercise it should be noted that the most accurate current measurements give it a mass of 91.1876. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734779be-1b1b-4bd5-9b5e-6c777cb7d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mom_limit = ...\n",
    "limited_missing_mom = ...\n",
    "histogram = plt.hist(limited_missing_mom.M, bins=bins, range=(lowerlimit,upperlimit), density = True)\n",
    "\n",
    "\n",
    "def breitwigner(E, gamma, M, a, b, A):\n",
    "    return a*E+b+A*( (2*np.sqrt(2)*M*gamma*np.sqrt(M**2*(M**2+gamma**2)))/(np.pi*np.sqrt(M**2+np.sqrt(M**2*(M**2+gamma**2)))) )/((E**2-M**2)**2+M**2*gamma**2)\n",
    "\n",
    "p0 = [5, 90, 4, 2, 4]\n",
    "y = histogram[0]\n",
    "x = 0.5*(histogram[1][0:-1] + histogram[1][1:])\n",
    "best, covariance = curve_fit(breitwigner, x, y, p0=p0, sigma=np.sqrt(y))\n",
    "error = np.sqrt(np.diag(covariance))\n",
    "    \n",
    "print(\"The values and the uncertainties from the optimization:\")\n",
    "print(\"\")\n",
    "print(f\"The value of the decay width = {best[0]} +- {error[0]}\")\n",
    "print(f\"The value of the maximum of the distribution = {best[1]} +- {error[1]}\")\n",
    "print(f\"a = {best[2]} +- {error[2]}\")\n",
    "print(f\"b = {best[3]} +- {error[3]}\")\n",
    "print(f\"A = {best[4]} +- {error[4]}\")\n",
    "\n",
    "plt.plot(x, breitwigner(x, *best), 'r-', label=f'gamma = {best[0]}, M = {best[1]}')\n",
    "plt.xlabel('Invariant mass [GeV]')\n",
    "plt.ylabel('Number of events')\n",
    "plt.title('The Breit-Wigner fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7b0c7-be99-4636-9b27-1b182c8886ab",
   "metadata": {},
   "source": [
    "It does seem to actually be more accurate, although it didn't make a big difference. This method of plotting invariant masses isnt't quite the best method for this. Let's now look at something called line width fit: Here we will plot ECM values instead of invariant mass values. Therefore we get a histogram that shows how many datapoints (meaning particles) we produce at each ECM values. Since LEP only produces Z bosons, the amount of Z bosons goes up as we get closer to its invariant mass and we should be able to measure it very accurately.  \n",
    "\n",
    "Since we are now relying on the number particles produced as a metric, the performace of the accelerator is also a variable. The performance of the accelerator can vary significantly and CERN measures it using a physical quantity called luminosity. Luminosity has a formula\n",
    "\n",
    "$$\n",
    "L = \\dfrac{1}{\\sigma} \\dfrac{dN}{dt}\n",
    "$$\n",
    "\n",
    "meaning it is the rate of events divided by cross-section. Cross-section describes the probability of the colliding particles actually causing a reaction. Therefore integraded luminosity is just the total number of events divided by the cross-section. The cross-section values are what we actually want to find and they can be calculated by dividing the number of events by the integrated luminosity. \n",
    "\n",
    "Here we will use integrated luminosity over the time that the accelerator run at a specific ECM. Since each center-of-mass energy (ECM) value has a different luminosity we need to find all ECM values and that can be done by using `.ECM.unique()` function on the dataset. From that we can see some values that are quite close to each other. This is because of noise in the measurement and can be fixed by setting the luminosity to all the ECM values less than 0.1 away from the exact value. Doing that we find that there are 7 different sections for ECM. Using the table given we can assign a luminosity to each datapoint in the dataset using `.loc`. The actual values for integrated luminosity can be found on papers published by CERN. An explanation on how the luminosity is measured can be found below.\n",
    "\n",
    "<details>\n",
    "<summary>Luminosity measurement</summary>  \n",
    "The actual formula for the luminosity of the machine is:\n",
    "    \n",
    "$$\n",
    "L = \\frac{n_1 n_2 f}{A}\n",
    "$$\n",
    "\n",
    "Here $n_1$ and $n_2$ are the number electrons and positrons in the accelerator, $f$ the frequency which they go around the the accelerator and $A$ the area of the beam. These quantities can be hard to measure accurately, so the result from this isn't the most accurate one we can get. Instead we use something called Bhabha scattering. It's a process where the colliding electron and positron produce a photon, which decays into an electron and a positron. For this process, the theoretical cross-section is known very accurately. Therefore we can measure the amount of events for this process in the accelerator and use the first formula to determine the luminosity. This gives us the luminosity for the accelerator. \n",
    "    \n",
    "![alt text](https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/Bhabha_scattering.png)\n",
    "\n",
    "</details>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212f232-866c-4918-9185-0dac4c472f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#section 1: \n",
    "lum1 = 629 #luminosity for this section\n",
    "condition1 = np.abs(dataset91.ECM-88.5) < 0.1 #ECM for this section is near 88.5\n",
    "dataset91.loc[condition1, \"LUM\"] = lum1 #set the luminosity under row \"LUM\"\n",
    "\n",
    "\n",
    "#section 2: \n",
    "lum2 = 647\n",
    "condition2 = np.abs(dataset91.ECM-89.5) < 0.1\n",
    "dataset91.loc[condition2, \"LUM\"] = lum2\n",
    "\n",
    "\n",
    "#section 3: \n",
    "lum3 = 633\n",
    "condition3 = np.abs(dataset91.ECM-90.2) < 0.1\n",
    "dataset91.loc[condition3 , \"LUM\"] = lum3\n",
    "\n",
    "\n",
    "#section 4: \n",
    "lum4 = 2274\n",
    "condition4 = np.abs(dataset91.ECM-91.2) < 0.1\n",
    "dataset91.loc[condition4, \"LUM\"] = lum4\n",
    "\n",
    "\n",
    "#section 5: \n",
    "lum5 = 680\n",
    "condition5 = np.abs(dataset91.ECM-92) < 0.1\n",
    "dataset91.loc[condition5, \"LUM\"] = lum5\n",
    "\n",
    "\n",
    "#section 6: \n",
    "lum6 = 634\n",
    "condition6 = np.abs(dataset91.ECM-93) < 0.1\n",
    "dataset91.loc[condition6, \"LUM\"] = lum6\n",
    "\n",
    "\n",
    "#section 7: \n",
    "lum7 = 492\n",
    "condition7 = np.abs(dataset91.ECM-93.7) < 0.1\n",
    "dataset91.loc[condition7, \"LUM\"] = lum7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5171f8e-3ff3-4bbc-95b1-38c474d84a66",
   "metadata": {},
   "source": [
    "There are actually a number of different ways to do this, some better than others. In the extra material below, we'll take a look at 2 of them, compare their time and space complexities. \n",
    "\n",
    "<details>\n",
    "<summary>Time and space complexity</summary>\n",
    "    \n",
    "Let's define 2 programs that accomplish the same task in different ways. The first one is identical to what we did on the previous cell. \n",
    "    \n",
    "```python\n",
    "\n",
    "def first_program(dataset):\n",
    "    #section 1: \n",
    "    lum1 = 711.1 #luminosity for this section\n",
    "    condition1 = np.abs(dataset.ECM-88.5) < 0.1 #ECM for this section is near 88.5\n",
    "    dataset.loc[condition1, \"LUM\"] = lum1 #set the luminosity under row \"LUM\"\n",
    "\n",
    "\n",
    "    #section 2: \n",
    "    lum2 = 632.7\n",
    "    condition2 = np.abs(dataset.ECM-89.5) < 0.1\n",
    "    dataset.loc[condition2, \"LUM\"] = lum2\n",
    "\n",
    "\n",
    "    #section 3: \n",
    "    lum3 = 622.6\n",
    "    condition3 = np.abs(dataset.ECM-90.2) < 0.1\n",
    "    dataset.loc[condition3 , \"LUM\"] = lum3\n",
    "\n",
    "\n",
    "    #section 4: \n",
    "    lum4 = 2482.5\n",
    "    condition4 = np.abs(dataset.ECM-91.2) < 0.1\n",
    "    dataset.loc[condition4, \"LUM\"] = lum4\n",
    "\n",
    "\n",
    "    #section 5: \n",
    "    lum5 = 666.1\n",
    "    condition5 = np.abs(dataset.ECM-92) < 0.1\n",
    "    dataset.loc[condition5, \"LUM\"] = lum5\n",
    "\n",
    "\n",
    "    #section 6: \n",
    "    lum6 = 634.6\n",
    "    condition6 = np.abs(dataset.ECM-93) < 0.1\n",
    "    dataset.loc[condition6, \"LUM\"] = lum6\n",
    "\n",
    "\n",
    "    #section 7: \n",
    "    lum7 = 681.2\n",
    "    condition7 = np.abs(dataset.ECM-93.7) < 0.1\n",
    "    dataset.loc[condition7, \"LUM\"] = lum7\n",
    "    \n",
    "    \n",
    "    \n",
    "def second_program(dataset):\n",
    "    dataset_sorted = dataset.sort_values(by=[\"ECM\"])\n",
    "    dataset_sorted = dataset_sorted.reset_index(drop=True)\n",
    "    unique_values = dataset_sorted.ECM.unique() #simulate having to print\n",
    "\n",
    "    #section 1: \n",
    "    index1 = dataset_sorted[dataset_sorted.ECM == 89.506].index[0]\n",
    "    lum1 = 711.1 #luminosity for this section\n",
    "    dataset_sorted.loc[:index1, \"LUM\"] = lum1 #set the luminosity under row \"LUM\"\n",
    "\n",
    "\n",
    "    #section 2: \n",
    "    lum2 = 632.7\n",
    "    index2 = dataset_sorted[dataset_sorted.ECM == 90.256].index[0]\n",
    "    dataset_sorted.loc[index1:index2, \"LUM\"] = lum2\n",
    "\n",
    "\n",
    "    #section 3: \n",
    "    lum3 = 622.6\n",
    "    index3 = dataset_sorted[dataset_sorted.ECM == 91.25].index[0]\n",
    "    dataset_sorted.loc[index2:index3, \"LUM\"] = lum3\n",
    "\n",
    "\n",
    "    #section 4: \n",
    "    lum4 = 2482.5\n",
    "    index4 = dataset_sorted[dataset_sorted.ECM == 92.004].index[0]\n",
    "    dataset_sorted.loc[index3:index4, \"LUM\"] = lum4\n",
    "\n",
    "\n",
    "    #section 5: \n",
    "    lum5 = 666.1\n",
    "    index5 = dataset_sorted[dataset_sorted.ECM == 93.015].index[0]\n",
    "    dataset_sorted.loc[index4:index5, \"LUM\"] = lum5\n",
    "\n",
    "\n",
    "    #section 6: \n",
    "    lum6 = 634.6\n",
    "    index6 = dataset_sorted[dataset_sorted.ECM == 93.765].index[0]\n",
    "    dataset_sorted.loc[index5:index6, \"LUM\"] = lum6\n",
    "\n",
    "\n",
    "    #section 7: \n",
    "    lum7 = 681.2\n",
    "    dataset_sorted.loc[index6:, \"LUM\"] = lum7\n",
    "    return dataset_sorted\n",
    "\n",
    "#measure first program in small dataset\n",
    "start_time = time.time()\n",
    "first_program(dataset91)\n",
    "end_time = time.time()\n",
    "result1 = end_time-start_time\n",
    "\n",
    "#measure second program in small dataset\n",
    "start_time = time.time()\n",
    "second_program(dataset91)\n",
    "end_time = time.time()\n",
    "result2 = end_time-start_time\n",
    "\n",
    "#measure first program in bigger dataset\n",
    "start_time = time.time()\n",
    "first_program(dataset)\n",
    "end_time = time.time()\n",
    "result3 = end_time-start_time\n",
    "\n",
    "#measure second program in bigger dataset\n",
    "start_time = time.time()\n",
    "second_program(dataset)\n",
    "end_time = time.time()\n",
    "result4 = end_time-start_time\n",
    "```\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "**Question:** \n",
    "\n",
    "Before we look at the results, which one do you think does better? Why?\n",
    "    \n",
    "<details>\n",
    "<summary>Results</summary>\n",
    "    \n",
    "First program with a small dataset: 0.018329620361328125\n",
    "    \n",
    "Second program with a small dataset: 0.016561031341552734\n",
    "    \n",
    "First program with a big dataset: 0.012064695358276367\n",
    "    \n",
    "Second program with a big dataset: 0.026291608810424805\n",
    "</details>\n",
    "\n",
    "<br/>\n",
    "\n",
    "This isn't the most efficient method of testing the speed of an algorithm, since it just takes the real times and subtracts them. It does work for basic testing and is much easier to use than the one used previously to test vectorisation. \n",
    "    \n",
    "So for the smaller dataset the first one is slower, but for the larger its always faster, why is this? Well lets look at the programs starting with the lines: \n",
    "    \n",
    "`condition1 = np.abs(dataset.ECM-88.5) < 0.1` program one\n",
    "\n",
    "`index1 = dataset[dataset.ECM == 89.478].index[0]` program two\n",
    "    \n",
    "Both are actually quite similar, running through the entire data checking the codition for each value. Let's say we have $N$ number of datapoints in our data. Then going through would take $aN$ time where $a$ is the amount of time it takes to go through one and is a constant. We usually denote this as $O(N)$ time complexity.  \n",
    "\n",
    "For the next lines: \n",
    "    \n",
    "`dataset.loc[condition1, \"LUM\"] = 711.1` program one\n",
    "\n",
    "`dataset.loc[:index1, \"LUM\"] = 711.1` program two\n",
    "    \n",
    "Here program one is actually worse. It goes through the entire data checking the whether the condition is True or False just like the last lines we looked at. This is slower than actually giving the indices that need to be edited. Since program 2 goes through all datapoints once, instead of 7 times its 7 times faster. Both programs are still $O(N)$ complexity since both runtimes grow linearly in terms of $N$, one is just always 7 times slower. \n",
    "\n",
    "But now the reason why program 2 is worse on a large dataset: It has to sort the data first. There are a bunch of different sorting algorithms, but this uses quicksort as a default which is $O(N log N)$. This is larger than any of the other parts of the program so when the amount of data grows this will grow faster than all the other parts explaining why for a larger dataset the second program is always slower. \n",
    "\n",
    "The last 2 parts which are the resetting of indices and calculation of unique values are both $O(N)$ for the same reasons we covered before. Now the complexity of the entire program is just the largest term which is $O(N log N)$.\n",
    "\n",
    "Now what about the memory usage? The tracemalloc library can be used to test the memory usage. Now when testing it is important to remember not to have anything already saved before for accurate measurements. The safest way to do that is to restart your kernel and run the first 2 cells to have everything you need imported and saved. After that you can run the cell below.    \n",
    "    \n",
    "```python\n",
    "tracemalloc.start()\n",
    "first_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "\n",
    "tracemalloc.start()\n",
    "second_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "```\n",
    "\n",
    "<br/>\n",
    "    \n",
    "**Question:**\n",
    "    \n",
    "Which one do you think does better? Why?\n",
    "    \n",
    "<details>\n",
    "<summary>Results</summary>\n",
    "    \n",
    "First program: (10153, 573705)\n",
    "    \n",
    "Second program: (155310, 16483973)\n",
    "    \n",
    "The first value given is the current memory usage of the program and the second the peak memory usage. The peak is what matters here since that is what can stop your program from running. Now clearly the first program is the better one, but why? If we look at what variables we actually save, its quite obvious. In the beginning we save the entire dataset, sorted to a different order and then save the entire dataset again just with different indices.\n",
    "    \n",
    "</details>\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "An easy way to make sure which part of the code is using too much memory is to compare just that part to the entire program.\n",
    "    \n",
    "```python\n",
    "tracemalloc.start()\n",
    "second_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "\n",
    "tracemalloc.start()\n",
    "dataset_sorted = dataset.sort_values([\"ECM\"])\n",
    "dataset_sorted = dataset_sorted.reset_index(drop=True)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "```\n",
    "    \n",
    "<br/>\n",
    "\n",
    "(303154, 16630625)\n",
    "    \n",
    "(4457122, 16482811)\n",
    "\n",
    "The peak is almost the same and current usage is ten times higher. This is clearly the main problem.\n",
    "    \n",
    "**Question:**\n",
    "    \n",
    "How would you improve the program?\n",
    "    \n",
    "<details>\n",
    "<summary>Some improvements</summary>\n",
    "    \n",
    "```python\n",
    "def second_program_improved(dataset):\n",
    "    dataset.sort_values(by=[\"ECM\"], inplace = True)\n",
    "    dataset.reset_index(drop=True, inplace = True)\n",
    "    unique_values = dataset.ECM.unique() #simulate having to print\n",
    "\n",
    "    #section 1:\n",
    "    lum1 = 711.1 #luminosity for this section\n",
    "    index1 = dataset[dataset.ECM == 89.506].index[0]\n",
    "    dataset.loc[:index1, \"LUM\"] = lum1 #set the luminosity under row \"LUM\"\n",
    "\n",
    "\n",
    "    #section 2: \n",
    "    lum2 = 632.7\n",
    "    index2 = dataset[dataset.ECM == 90.256].index[0]\n",
    "    dataset.loc[index1:index2, \"LUM\"] = lum2\n",
    "\n",
    "\n",
    "    #section 3: \n",
    "    lum3 = 622.6\n",
    "    index3 = dataset[dataset.ECM == 91.25].index[0]\n",
    "    dataset.loc[index2:index3, \"LUM\"] = lum3\n",
    "\n",
    "\n",
    "    #section 4: \n",
    "    lum4 = 2482.5\n",
    "    index4 = dataset[dataset.ECM == 92.004].index[0]\n",
    "    dataset.loc[index3:index4, \"LUM\"] = lum4\n",
    "\n",
    "\n",
    "    #section 5: \n",
    "    lum5 = 666.1\n",
    "    index5 = dataset[dataset.ECM == 93.015].index[0]\n",
    "    dataset.loc[index4:index5, \"LUM\"] = lum5\n",
    "\n",
    "\n",
    "    #section 6: \n",
    "    lum6 = 634.6\n",
    "    index6 = dataset[dataset.ECM == 93.765].index[0]\n",
    "    dataset.loc[index5:index6, \"LUM\"] = lum6\n",
    "\n",
    "\n",
    "    #section 7: \n",
    "    lum7 = 681.2\n",
    "    dataset.loc[index6:, \"LUM\"] = lum7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tracemalloc.start()\n",
    "second_program(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "\n",
    "tracemalloc.start()\n",
    "second_program_improved(dataset)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "```\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "(266297, 15801264)\n",
    "    \n",
    "(4647288, 12208766)\n",
    "    \n",
    "Copying the dataset over and over again is very inefficient. Here we just edit the dataset instead. It doesn't fix the problem of having to sort, which is often quite bad for memory, but does significantly improve the program. It's important to remember that now this function will change the dataset that we give it and sometimes this is not adviced. However, generally when dealing with large sets of data, it's recommended to edit the existing dataset instead of creating a copy to edit, unless you know that you will need the original again.\n",
    "    \n",
    "</details>\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20137d-89fa-47b9-ac85-9fe3a7877974",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise 5: Line-width fit\n",
    "\n",
    "Now let's plot the histogram. Here you will have to find the correct range and number of bins to get the best possible graph for fitting. You want to have a graph with no empty bars as they can mess up the fit. You will also have to figure out how to correct for the luminosity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2c97a-6fa0-4ff9-a1bf-aeee70589f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerlimit = ...\n",
    "upperlimit = ...\n",
    "bins= ...\n",
    "\n",
    "#remember to correct for the luminosity\n",
    "histogram = ...\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('ECM', fontsize=15)\n",
    "plt.ylabel('Number of events (normalised)', fontsize=15)\n",
    "plt.title('Number of events of each ECM \\n', fontsize=15) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb29ea-59cb-4a44-aace-2614367bf267",
   "metadata": {},
   "source": [
    "Now we fit the Breit-Wigner function just like before. This part you have done already once so just run the code normally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ddc1c-60ca-4f4d-b5d9-966c268e6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = [1, 91, 1, 3, 1]\n",
    "y = histogram[0]\n",
    "x = 0.5*(histogram[1][0:-1] + histogram[1][1:])\n",
    "best, covariance = curve_fit(breitwigner, x, y, p0=p0, sigma=np.sqrt(y)) \n",
    "error = np.sqrt(np.diag(covariance))\n",
    "\n",
    "print(\"The values and the uncertainties from the optimization:\")\n",
    "print(\"\")\n",
    "print(f\"The value of the decay width = {best[0]} +- {error[0]}\")\n",
    "print(f\"The value of the maximum of the distribution = {best[1]} +- {error[1]}\")\n",
    "print(f\"a = {best[2]} +- {error[2]}\")\n",
    "print(f\"b = {best[3]} +- {error[3]}\")\n",
    "print(f\"A = {best[4]} +- {error[4]}\")\n",
    "\n",
    "#For the smoother fit:\n",
    "more_points = np.arange(lowerlimit,upperlimit, step=0.2)\n",
    "\n",
    "plt.hist(dataset91.ECM, bins=bins, range=(lowerlimit,upperlimit), weights = 1/dataset91.LUM, density = True)\n",
    "plt.plot(more_points, breitwigner(more_points, *best), 'r-', label=f'gamma = {best[0]}, M = {best[1]}')\n",
    "plt.xlabel('ECM [GeV]')\n",
    "plt.ylabel('Number of events (normalised)')\n",
    "plt.title('The Breit-Wigner fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3006e86-df2f-4b79-9b0c-7e88ff834293",
   "metadata": {},
   "source": [
    "Now let's compare to modern CMS data. We'll do the same steps as for the LEP data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ca023-f628-45e3-b305-8c388921f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CMS = pd.read_csv('https://opendata.cern.ch/record/5201/files/Dimuon_DoubleMu.csv')\n",
    "                        \n",
    "lowerlimit = 70\n",
    "upperlimit = 100\n",
    "bins = 50\n",
    "\n",
    "#Change the dataset to be only that area\n",
    "limitedmasses_CMS = dataset_CMS[(dataset_CMS.M > lowerlimit) & (dataset_CMS.M < upperlimit)]\n",
    "histogram = plt.hist(limitedmasses_CMS.M, bins=bins, range=(lowerlimit,upperlimit), density = True) \n",
    "plt.close() #stop from showing the plot\n",
    "\n",
    "#Initial guesses:\n",
    "p0 = [5, 90, 4, 2, 4]\n",
    "                      \n",
    "\n",
    "#Histogram as an object consists of 2 lists: List of the heights of the bars and the edges of the bins\n",
    "#Using normal list operations we can use the histogram to get x and y coordinates:\n",
    "y = histogram[0] #gives the height of each bar in a list\n",
    "x = 0.5*(histogram[1][0:-1] + histogram[1][1:]) #list of midpoints of a bins\n",
    "\n",
    "#Fits the defined Breit-Wigner function to the (x,y) points gotten from the histogram\n",
    "best, covariance = curve_fit(breitwigner, x, y, p0=p0, sigma=np.sqrt(y)) #sigma is the magnitude of the uncertainty in y-data\n",
    "#Covariance is is determined using the sigma given. The diagonal elements are the variances.\n",
    "error = np.sqrt(np.diag(covariance))\n",
    "    \n",
    "#Parameters and their errors from optimization:\n",
    "print(\"The values and the uncertainties from the optimization:\")\n",
    "print(\"\")\n",
    "print(f\"The value of the decay width = {best[0]} +- {error[0]}\")\n",
    "print(f\"The value of the maximum of the distribution = {best[1]} +- {error[1]}\")\n",
    "print(f\"a = {best[2]} +- {error[2]}\")\n",
    "print(f\"b = {best[3]} +- {error[3]}\")\n",
    "print(f\"A = {best[4]} +- {error[4]}\")\n",
    "\n",
    "#plot both graphs:\n",
    "plt.hist(limitedmasses_CMS.M, bins=bins, range=(lowerlimit,upperlimit), density = True)\n",
    "plt.plot(x, breitwigner(x, *best), 'r-', label=f'gamma = {best[0]}, M = {best[1]}')\n",
    "plt.xlabel('Invariant mass [GeV]')\n",
    "plt.ylabel('Number of events (normalised)')\n",
    "plt.title('The Breit-Wigner fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c68bc-e9e0-4cda-83c4-747c5961b3f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Question:**\n",
    "\n",
    "Which one is better? Why is it better?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>  \n",
    "If you look at the results and compare them to the mass you can find on google you can see that LEP is closer to the correct result. The LHC, or hadron colliders altogether aren't as good at measuring the mass of the Z boson as LEP was. LEP produces only Z bosons, whereas LHC produces all kinds of particles. Electron-positron colliders are simply better at some tasks, which is why CERN is actually planning on making a new electron-positron collider as the next circular collider. \n",
    "</details>     \n",
    "\n",
    "</br>\n",
    "\n",
    "We can actually make the CMS data more accurate using pseudorapidities. Pseudorapidity is a quantity that describes the angle of the detected particle. Its infinite when the particle goes to the direction of the beam pipe and zero if the particle goes directly at the wall of the beam pipe. Going perpendicular to the wall means the particle will have less distance to go through. When the particle is going through, it scatters randomly, affecting its trajectory which causes inaccuracies in the measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034029e-50a0-465f-b200-be716f8b5173",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise 6: Pseudorapidity\n",
    "\n",
    "Remove bad measurements by choosing a which pseudorapidities to keep. You may also have to alter the histogram to account fot the smaller number of datapoints. See how close you can get to the actual value. You will have to add two inequalities inside the brackets of the first line. This can be done using & symbol: `dataset[(condition1) & (condition2)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c398347-d365-4d72-8381-6e0e81c2f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_etas = dataset_CMS[...]\n",
    "\n",
    "#change bins and limits to still get a good plot:\n",
    "bins= ...\n",
    "lowerlimit = ...\n",
    "upperlimit = ...\n",
    "histogram = plt.hist(small_etas.M, bins=bins, range=(lowerlimit,upperlimit), density = True) \n",
    "plt.close() \n",
    "p0 = [5, 90, 4, 2, 4]\n",
    "                      \n",
    "y = histogram[0] \n",
    "x = 0.5*(histogram[1][0:-1] + histogram[1][1:]) \n",
    "\n",
    "best, covariance = curve_fit(breitwigner, x, y, p0=p0, sigma=np.sqrt(y)) \n",
    "error = np.sqrt(np.diag(covariance))\n",
    "    \n",
    "print(\"The values and the uncertainties from the optimization:\")\n",
    "print(\"\")\n",
    "print(f\"The value of the decay width = {best[0]} +- {error[0]}\")\n",
    "print(f\"The value of the maximum of the distribution = {best[1]} +- {error[1]}\")\n",
    "print(f\"a = {best[2]} +- {error[2]}\")\n",
    "print(f\"b = {best[3]} +- {error[3]}\")\n",
    "print(f\"A = {best[4]} +- {error[4]}\")\n",
    "\n",
    "plt.hist(small_etas.M, bins=bins, range=(lowerlimit,upperlimit), density = True)\n",
    "plt.plot(x, breitwigner(x, *best), 'r-', label=f'gamma = {best[0]}, M = {best[1]}')\n",
    "plt.xlabel('Invariant mass [GeV]')\n",
    "plt.ylabel('Number of events (normalised)')\n",
    "plt.title('The Breit-Wigner fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
