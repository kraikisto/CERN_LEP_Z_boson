{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae64a021-96c9-42b9-b63d-456997e84868",
   "metadata": {},
   "source": [
    "Start with the necessary imports and reading the datasets of github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f60f8-9f6d-4a70-bb94-09923dbb9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dataset91 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short91_f1_000.csv.gz',index_col=False)\n",
    "dataset92 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short92_e2_000.csv.gz',index_col=False)\n",
    "dataset93 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short93_d2_000.csv.gz',index_col=False)\n",
    "dataset94 = pd.read_csv('https://raw.githubusercontent.com/kraikisto/CERN_LEP_Z_boson/main/dimuon_short94_c2_000.csv.gz',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6431d-4a28-4f4f-a14c-19c0a0c4d9a4",
   "metadata": {},
   "source": [
    "The goal of this exercise is to find a particle using invariant mass. This can be done by checking which invariant masses are produced by the particle accelerator. If there area a lot of datapoints for a specific invariant mass that means there's a particle. Since the dataset does not have invariant mass, we have to calculate it ourselves. To do that we first need pseudorapidities for both particles which we can calculate from formula:\n",
    "\n",
    "$$\n",
    "\\eta = -\\ln(\\tan(\\frac{\\theta}{2}))\n",
    "$$\n",
    "\n",
    "\n",
    "Now using pseudorapidities we can calculate the invariant mass:\n",
    "\n",
    "$$\n",
    "M = \\sqrt{2p_{T1}p_{T2}( \\cosh(\\eta_1-\\eta_2)-\\cos(\\phi_1-\\phi_2) )}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e98b5-ef9b-4126-9f17-2776a15b0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset91[\"ETA1\"] = - np.log(np.tan(dataset91.THETA1/2))\n",
    "dataset91[\"ETA2\"] = - np.log(np.tan(dataset91.THETA2/2))\n",
    "dataset91[\"M\"] = np.sqrt(2*dataset91.PT1*dataset91.PT2*(np.cosh(dataset91.ETA1 - dataset91.ETA2) - np.cos(dataset91.PHI1 - dataset91.PHI2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed369d2-2d3c-4a3f-befa-0519d9287db7",
   "metadata": {},
   "source": [
    "Now we need to define the integrated luminosity for each event. Luminosity is defined as\n",
    "\n",
    "$$\n",
    "L = \\dfrac{1}{\\sigma} \\dfrac{dN}{dt}\n",
    "$$\n",
    "\n",
    "meaning it is the rate of events divided by cross-section. Cross-section describes the probability of the colliding particles actually causing a reaction. Therefore integraded luminosity is just the total number of events divided by the cross-section. Because the luminosity can be different for different values of center-of-mass energy (ECM) we would get a weighted diagram since a large luminosity naturally produces a lot of results meaning a lot of invariant mass values. This could distrubt our ability to recognise which invariant mass values are actually produced often. Therefore we want to calculate which invariatn mass values are common in proportion to the amount of reactions we have. This can be done just by weighting the histogram by the inverse of luminosity. \n",
    "\n",
    "Since each center-of-mass energy (ECM) value has a different luminosity we need to find all ECM values and that can be done by using .ECM.unique() function on the dataset. From that we can see some values that are quite close to each other. This is beccause of noise in the measurement and can be ignored. Doing that we find that there are 7 different values for ECM. Using the table given we can assing a luminosity to each datapoint in the dataset by doing the following. This is done by first sorting the dataset by ECM, then finding the point in which ECM value changes using .index[0]. Lastly the values can be assigned using .loc where we can specify rows and columns which we want to assign the value to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d153510-cb21-47e7-8935-bc8c0c519172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#values sorted by ECM (for convenience of programming)\n",
    "dataset91 = dataset91.sort_values(['ECM']) \n",
    "dataset91 = dataset91.reset_index(drop = True) #correct index after sort\n",
    "\n",
    "#section 1: \n",
    "slice_index1 = dataset91[dataset91.ECM == 89.506].index[0] \n",
    "lum1 = 711.1 #luminosity for this section\n",
    "dataset91.loc[0:slice_index1-1, \"LUM\"] = lum1\n",
    "\n",
    "#section 2: \n",
    "slice_index2 = dataset91[dataset91.ECM == 90.256].index[0] \n",
    "lum2 = 632.7 #luminosity for this section\n",
    "dataset91.loc[slice_index1:slice_index2-1, \"LUM\"] = lum2\n",
    "\n",
    "#section 3: \n",
    "slice_index3 = dataset91[dataset91.ECM == 91.25].index[0] \n",
    "lum3 = 622.6 #luminosity for this section\n",
    "dataset91.loc[slice_index2:slice_index3-1, \"LUM\"] = lum3\n",
    "\n",
    "#section 4: \n",
    "slice_index4 = dataset91[dataset91.ECM == 92.004].index[0] \n",
    "lum4 = 2482.5 #luminosity for this section\n",
    "dataset91.loc[slice_index3:slice_index4-1, \"LUM\"] = lum4\n",
    "\n",
    "#section 5: \n",
    "slice_index5 = dataset91[dataset91.ECM == 93.015].index[0] \n",
    "lum5 = 666.1 #luminosity for this section\n",
    "dataset91.loc[slice_index4:slice_index5-1, \"LUM\"] = lum5\n",
    "\n",
    "#section 6: \n",
    "slice_index6 = dataset91[dataset91.ECM == 93.765].index[0] \n",
    "lum6 = 634.6 #luminosity for this section\n",
    "dataset91.loc[slice_index5:slice_index6-1, \"LUM\"] = lum6\n",
    "\n",
    "#section 7: \n",
    "lum7 = 681.2 #luminosity for this section\n",
    "dataset91.loc[slice_index6:, \"LUM\"] = lum7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf197d91-1a4b-4315-8191-ea8f77142fe4",
   "metadata": {},
   "source": [
    "Important note: Making copies of each section, changing them and then creating a new dataset from the copies is incredibly inefficient, but much easier to understand in my opinion. Since we are making something that could be used for education, I'm going to write this method in this comment for now. It's far easier to understand than .loc which is just a niche feature of pandas. The result is exactly the same.\n",
    "\n",
    "section 1:\n",
    "\n",
    "slice_index1 = dataset91[dataset91.ECM == 89.506].index[0]\n",
    "\n",
    "section1 = dataset91[0:slice_index1]\n",
    "\n",
    "lum_lst88 = 711.1 #luminosity for this section\n",
    "\n",
    "section1[\"Normalised\"] = np.divide(section1.M, lum_lst88)\n",
    "\n",
    "at the end combine sections:\n",
    "\n",
    "dataset91 = [section1, section2, section3,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53eb5d9-3d3a-4411-9050-7fc990eb6840",
   "metadata": {},
   "source": [
    "Next we create a histogram of the invariant masses. As previously covered the historgram should be weighted 1/luminosity. Additionally I will normalise the graph. This is only the make it easier to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7d5f8-f85d-4313-b8e6-7df07b6ce949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted by luminosity\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.hist(dataset91.M, bins=100, range=(0,150), weights = 1/dataset91.LUM, density = True)\n",
    "\n",
    "plt.xlabel('Invariant mass [GeV/cÂ²]', fontsize=15)\n",
    "plt.ylabel('Number of events (normalised)', fontsize=15)\n",
    "plt.title('Invariant mass normalised by luminosity \\n', fontsize=15) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c037d99-65fa-4612-826b-b678c1fc1db2",
   "metadata": {},
   "source": [
    "Now that we have the histogram we can start analysing it. There is a small peak at around 103 GeV. This peak is actually quite small and not all that significant, but it could be because we only use 1991 data. Let's just assume that nothing has been done wrong so far and this peak signifies a particle. We can actually get more from this histogram. This is done by plotting what is called a Breit-Wigner function on the histogram. Breit-Wigner distribution is of the following form:\n",
    "\n",
    "$$\n",
    "f(E) = \\dfrac{k}{(E^2 - M^2)^2 + M^2 \\Gamma^2} \n",
    "$$\n",
    "\n",
    "where $k = \\dfrac{2\\sqrt{2}M \\Gamma \\gamma}{\\pi\\sqrt{M^2 + \\gamma}}$ with $\\gamma = \\sqrt{M^2(M^2 + \\Gamma^2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb8e32-d4a7-428c-8a02-f17cdcf5a8b2",
   "metadata": {},
   "source": [
    "Let's start with the simple parts of the fitting. First we only cover the area around where we know the particle is by setting limits to the mass values we use and remove the rest from the dataset. Then we set the initial guesses. In this fit there are 5 variables. We can guess that the max values is the peak we saw in the histogram. The rest we can guess and if the plot isn't good try again. (Are they always positive? It seems so by testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ca375-e69d-442a-ab8c-074077b73725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit to where Z-boson should be:\n",
    "lowerlimit = 70\n",
    "upperlimit = 110\n",
    "\n",
    "\n",
    "#Change the dataset for that area\n",
    "limitedmasses = dataset91[(dataset91.M > lowerlimit) & (dataset91.M < upperlimit)]\n",
    "histogram = plt.hist(limitedmasses.M, bins=50, range=(lowerlimit,upperlimit), weights = 1/limitedmasses.LUM, density = True)\n",
    "plt.close() #stop from showing the plot\n",
    "\n",
    "#Initial guesses:\n",
    "initials = [3, 100, 4, 2, 100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50b745-d955-4475-8221-3078848fc7f9",
   "metadata": {},
   "source": [
    "Next step is to create the fit. First we define a function that matches the form of the Breit-Wigner function given earlier. Then we fit this function into the histogram by using curve_fit() from scipy.optimize. Let's also save the best values and covariance that the fitting gives us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34246707-1005-48d4-8283-7f84e6ad577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def breitwigner(E, gamma, M, a, b, A):\n",
    "    return a*E+b+A*( (2*np.sqrt(2)*M*gamma*np.sqrt(M**2*(M**2+gamma**2)))/(np.pi*np.sqrt(M**2+np.sqrt(M**2*(M**2+gamma**2)))) )/((E**2-M**2)**2+M**2*gamma**2)\n",
    "\n",
    "#use the histogram to make a function:\n",
    "y = histogram[0]\n",
    "x = 0.5*(histogram[1][0:-1] + histogram[1][1:])\n",
    "best, covariance = curve_fit(breitwigner, x, y, p0=initials, sigma=np.sqrt(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56cd741-4b16-4c5f-b5d3-370030af9494",
   "metadata": {},
   "source": [
    "Lastly we plot both this fitted function and the histogram into the same picture. We can also print the best values and their limits of error, which can be easily calculated from the covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b04f71-40b9-41c6-8464-267bca95083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.sqrt(np.diag(covariance))\n",
    "    \n",
    "#Parameters and their errors from optimization:\n",
    "print(\"The values and the uncertainties from the optimization:\")\n",
    "print(\"\")\n",
    "print(\"The value of the decay width = {} +- {}\".format(best[0], error[0]))\n",
    "print(\"The value of the maximum of the distribution = {} +- {}\".format(best[1], error[1]))\n",
    "print(\"a = {} +- {}\".format(best[2], error[2]))\n",
    "print(\"b = {} +- {}\".format(best[3], error[3]))\n",
    "print(\"A = {} +- {}\".format(best[4], error[4]))\n",
    "\n",
    "#plot both graphs:\n",
    "plt.hist(limitedmasses.M, bins=50, range=(lowerlimit,upperlimit), weights = 1/limitedmasses.LUM, density = True)\n",
    "plt.plot(x, breitwigner(x, *best), 'r-', label='gamma = {}, M = {}'.format(best[0], best[1]))\n",
    "plt.xlabel('Invariant mass [GeV]')\n",
    "plt.ylabel('Number of events (normalised)')\n",
    "plt.title('The Breit-Wigner fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef82b7-31dd-4af1-b897-56bb9d31364d",
   "metadata": {},
   "source": [
    "Mostly because I was interested to see what would happen here is the same plot but with half of the datapoints missing. I removed all the ones with large pseudorapidities since they are less accurate. Does not seem to change much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2652be6-d398-40e6-9c53-cc6fab118266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudorapidity_limit = 0.5\n",
    "print(len(limitedmasses))\n",
    "limited_pseudorapidies = limitedmasses[(abs(limitedmasses.ETA1) < 0.05) & (abs(limitedmasses.ETA2) < 0.05)]\n",
    "print(len(limited_pseudorapidies))\n",
    "histogram = plt.hist(limitedmasses.M, bins=50, range=(lowerlimit,upperlimit), density = True)\n",
    "\n",
    "\n",
    "def breitwigner(E, gamma, M, a, b, A):\n",
    "    return a*E+b+A*( (2*np.sqrt(2)*M*gamma*np.sqrt(M**2*(M**2+gamma**2)))/(np.pi*np.sqrt(M**2+np.sqrt(M**2*(M**2+gamma**2)))) )/((E**2-M**2)**2+M**2*gamma**2)\n",
    "\n",
    "#Initial guesses:\n",
    "initials = [1.5, 100, 0, 2, 100]\n",
    "\n",
    "#use the histogram to make a function:\n",
    "y = histogram[0]\n",
    "x = 0.5*(histogram[1][0:-1] + histogram[1][1:])\n",
    "best, covariance = curve_fit(breitwigner, x, y, p0=initials, sigma=np.sqrt(y))\n",
    "error = np.sqrt(np.diag(covariance))\n",
    "    \n",
    "#Values from optimization:\n",
    "print(\"The values and the uncertainties from the optimization:\")\n",
    "print(\"\")\n",
    "print(\"The value of the decay width = {} +- {}\".format(best[0], error[0]))\n",
    "print(\"The value of the maximum of the distribution = {} +- {}\".format(best[1], error[1]))\n",
    "print(\"a = {} +- {}\".format(best[2], error[2]))\n",
    "print(\"b = {} +- {}\".format(best[3], error[3]))\n",
    "print(\"A = {} +- {}\".format(best[4], error[4]))\n",
    "\n",
    "\n",
    "#plot graph:\n",
    "plt.plot(x, breitwigner(x, *best), 'r-', label='gamma = {}, M = {}'.format(best[0], best[1]))\n",
    "plt.xlabel('Invariant mass [GeV]')\n",
    "plt.ylabel('Number of events (normalised)')\n",
    "plt.title('The Breit-Wigner fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb96a90-afce-4658-b802-e001f9e9572f",
   "metadata": {},
   "source": [
    "What if we remove high values of missing potential? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2b2cd-2bc3-4c30-b52b-75d74377dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
